
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>Jianyu</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="PengYuan">
    

    
    <meta property="og:type" content="website">
<meta property="og:title" content="Jianyu">
<meta property="og:url" content="https://pengyuanqiuqiu.github.io/index.html">
<meta property="og:site_name" content="Jianyu">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jianyu">

    
    <link rel="alternative" href="http://www.163.com/rss/" title="Jianyu" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="Jianyu" title="Jianyu"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Jianyu">Jianyu</a></h1>
				<h2 class="blog-motto">Learn more and more</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:pengyuanqiuqiu.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/17/pca与lda的区别和联系/" title="PCA与LDA的区别和联系" itemprop="url">PCA与LDA的区别和联系</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-17T08:22:26.710Z" itemprop="datePublished"> 发表于 2018-07-17</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>1、PCA无需样本标签，属于<strong>无监督学习降维</strong>；LDA需要样本标签，属于<strong>有监督学习降维</strong>。二者均是寻找一定的特征向量w来降维的，其中，LDA抓住样本的判别特征，PCA则侧重描叙特征。概括来说，PCA选择样本点投影具有最大方差的方向，LDA选择分类性能最好的方向。</p>
<p>2、PCA降维是直接和特征维度相关的，比如原始数据是d维的，那么PCA后，可以任意选取1维、2维，一直到d维都行（当然是对应特征值大的那些）。LDA降维是直接和类别的个数C相关的，与数据本身的维度没关系，比如原始数据是d维的，一共有C个类别，那么LDA降维之后，一般就是1维，2维到C-1维进行选择（当然对应的特征值也是最大的一些）。要求降维后特征向量维度大于C-1的，不能使用LDA。<br>     对于很多两类分类的情况，LDA之后就剩下1维，找到分类效果最好的一个阈值貌似就可以了。举个例子，假设图象分类，两个类别正例反例，每个图象10000维特征，那么LDA之后，就只有1维特征，并且这维特征的分类能力最好。</p>
<p>3、PCA投影的坐标系都是正交的，而LDA根据类别的标注关注分类能力，因此不保证投影到的坐标系是正交的（一般都不正交）</p>
<p>LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。</p>
<p>　　　　首先我们看看相同点：</p>
<p>　　　　1）两者均可以对数据进行降维。</p>
<p>　　　　2）两者在降维时均使用了矩阵特征分解的思想。</p>
<p>　　　　3）两者都假设数据符合高斯分布。</p>
<p>　　　　我们接着看看不同点：</p>
<p>　　　　1）LDA是有监督的降维方法，而PCA是无监督的降维方法</p>
<p>　　　　2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。</p>
<p>　　　　3）LDA除了可以用于降维，还可以用于分类。</p>
<p>　　　　4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/17/异常值分析/" title="异常值分析" itemprop="url">异常值分析</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-17T07:54:20.197Z" itemprop="datePublished"> 发表于 2018-07-17</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="异常值是什么？"><a href="#异常值是什么？" class="headerlink" title="异常值是什么？"></a>异常值是什么？</h3><p>残差很大的点，<strong>即在数据集中存在不合理的值，又称离群点</strong>；</p>
<h3 id="缺失值产生的原因"><a href="#缺失值产生的原因" class="headerlink" title="缺失值产生的原因"></a>缺失值产生的原因</h3><p>每当我们遇到异常值时，处理这些异常值的理想方法就是找出引起这些异常值的原因。 处理它们的方法将取决于它们发生的原因， 异常值的原因可以分为两大类：</p>
<blockquote>
<ul>
<li>人为错误</li>
<li>自然错误</li>
</ul>
</blockquote>
<h4 id="数据输入错误"><a href="#数据输入错误" class="headerlink" title="数据输入错误"></a>数据输入错误</h4><p>人为错误（如数据收集，记录或输入过程中导致的错误）会导致数据中的异常值。 例如：客户的年收入是10万美元，但意外地，数据输入操作附加一个零。 现在的收入就是100万美元，是10倍。 显然，与其他人相比，这将是异常值。</p>
<h4 id="测量误差"><a href="#测量误差" class="headerlink" title="测量误差"></a>测量误差</h4><p>这是异常值最常见的来源。 当使用的测量仪器出现故障时，会引起这种情况。 例如：有10台称重机。 其中9个是正确的，1个是错误的。 故障机器上的人员测量重量将高于或低于组内其余人员。 在故障机器上测量的重量可能导致异常值。</p>
<h4 id="实验误差"><a href="#实验误差" class="headerlink" title="实验误差"></a>实验误差</h4><p>异常值的另一个原因是实验误差。 例如：在7名跑步者的100米冲刺中，有一名选手错过了跑的口令，让他开始延迟。 因此，这使得跑步者的跑步时间比其他跑步者要多， 总运行时间可能是一个异常值。</p>
<h4 id="故意异常值"><a href="#故意异常值" class="headerlink" title="故意异常值"></a>故意异常值</h4><p>通常在自我报告的措施中涉及敏感数据。 例如：通常青少年报告酒量，只有其中一小部分报告实际价值，这里的实际值可能看起来像异常值，因为其余的青少年正在假值。</p>
<h4 id="数据处理错误"><a href="#数据处理错误" class="headerlink" title="数据处理错误"></a>数据处理错误</h4><p>无论何时执行数据挖掘，我们从多个来源提取数据。 某些操作或提取错误可能会导致数据集中的异常值。</p>
<h4 id="抽样错误"><a href="#抽样错误" class="headerlink" title="抽样错误"></a>抽样错误</h4><p>例如，衡量运动员的身高，错误地在样品中包括几名篮球运动员。 这种包含可能会导致数据集中的异常值。</p>
<h4 id="自然异常值"><a href="#自然异常值" class="headerlink" title="自然异常值"></a>自然异常值</h4><p>当异常值不是人为的（由于错误），它是一个自然的异常值。 例如：注意到其中一家著名的保险公司，前50名财务顾问的表现远远高于其他人。 令人惊讶的是，这不是由于任何错误。 因此，每当与顾问一起执行任何数据挖掘活动时，我们都会分别对待此细分。</p>
<h2 id="异常值分析"><a href="#异常值分析" class="headerlink" title="异常值分析"></a>异常值分析</h2><ul>
<li><p>基于统计的方法</p>
</li>
<li><p>基于距离的方法</p>
</li>
<li><p>基于密度的方法</p>
</li>
<li><p>基于聚类的方法</p>
</li>
<li><p>基于分类的方法</p>
</li>
<li><p>基于深度的方法</p>
</li>
<li><p>基于小波变换的方法</p>
</li>
<li><p>基于图的方法</p>
</li>
<li><p>基于模式的方法</p>
</li>
<li><p>基于神经网络的方法</p>
</li>
</ul>
<p>1.简单统计分析<br>    对属性值进行一个描述性的统计，从而查看哪些值是不合理的。比如对年龄这个属性进行规约：年龄的区间在[0:200]，如果样本中的年龄值不再该区间范围内，则表示该样本的年龄属性属于异常值。</p>
<ol>
<li>3δ原则<br>当数据服从正态分布：<br> 根据正态分布的定义可知，距离平均值3δ之外的概率为 P(|x-μ|&gt;3δ) &lt;= 0.003 ，这属于极小概率事件，在默认情况下我们可以认定，距离超过平均值3δ的样本是不存在的。 因此，当样本距离平均值大于3δ，则认定该样本为异常值。</li>
</ol>
<p>这里写图片描述<br>当数据不服从正态分布：<br>当数据不服从正态分布，可以通过远离平均距离多少倍的标准差来判定，多少倍的取值需要根据经验和实际情况来决定。<br>3．箱型图分析<br>箱型图提供了一个识别异常值的标准，即大于或小于箱型图设定的上下界的数值即为异常值，箱型图如下图所示：<br>这里写图片描述<br>首先我们定义下上四分位和下四分位。<br>上四分位我们设为 U，表示的是所有样本中只有1/4的数值大于U<br>同理，下四分位我们设为 L，表示的是所有样本中只有1/4的数值小于L<br>那么，上下界又是什么呢？<br>我们设上四分位与下四分位的插值为IQR，即：IQR=U-L<br>那么，上界为 U+1.5IQR ，下界为： L - 1.5IQR<br>箱型图选取异常值比较客观，在识别异常值方面有一定的优越性。</p>
<p><strong>异常值的处理方法常用有四种</strong>：</p>
<p>1.删除含有异常值的记录</p>
<p>2.将异常值视为缺失值，交给缺失值处理方法来处理</p>
<p>3.用平均值来修正</p>
<p>4.不处理</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/11/随机森林之特征选择/" title="随机森林之特征选择(转)" itemprop="url">随机森林之特征选择(转)</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-11T01:35:05.563Z" itemprop="datePublished"> 发表于 2018-07-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p><strong>摘要：</strong>在<a href="http://www.cnblogs.com/justcxtoworld/p/3420258.html" target="_blank" rel="noopener">随机森林介绍</a>中提到了随机森林一个重要特征：能够计算单个特征变量的重要性。并且这一特征在很多方面能够得到应用,例如在银行贷款业务中能否正确的评估一个企业的信用度,关系到是否能够有效地回收贷款。但是信用评估模型的数据特征有很多,其中不乏有很多噪音,所以需要计算出每一个特征的重要性并对这些特征进行一个排序,进而可以从所有特征中选择出重要性靠前的特征。</p>
<h4 id="一：特征重要性"><a href="#一：特征重要性" class="headerlink" title="一：特征重要性"></a>一：特征重要性</h4><p>在随机森林中某个特征X的重要性的<strong>计算方法</strong>如下：</p>
<p>1：对于随机森林中的每一颗决策树,使用相应的<a href="http://www.cnblogs.com/justcxtoworld/p/3434266.html" target="_blank" rel="noopener">OOB</a>(袋外数据)数据来计算它的<a href="http://www.cnblogs.com/justcxtoworld/p/3434266.html" target="_blank" rel="noopener">袋外数据误差</a>,记为<strong>errOOB1</strong>.</p>
<p>2:  随机地对袋外数据OOB所有样本的特征X加入噪声干扰(就可以随机的改变样本在特征X处的值),再次计算它的袋外数据误差,记为<strong>errOOB2</strong>.</p>
<p>3：假设随机森林中有Ntree棵树,那么对于<strong>特征X的重要性=∑(errOOB2-errOOB1)/Ntree</strong>,之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后,袋外的准确率大幅度降低,则说明这个特征对于样本的分类结果影响很大,也就是说它的重要程度比较高。</p>
<h4 id="二：特征选择"><a href="#二：特征选择" class="headerlink" title="二：特征选择"></a>二：特征选择</h4><p>在论文 <a href="http://hal.archives-ouvertes.fr/docs/00/75/54/89/PDF/PRLv4.pdf" target="_blank" rel="noopener">Variable Selection using Random Forests</a>中详细的论述了基于随机森林的特征选择方法,这里我们进行一些回顾。</p>
<p>首先特征选择的<strong>目标</strong>有两个：</p>
<p>1：找到与应变量高度相关的特征变量。</p>
<p>2：选择出数目较少的特征变量并且能够充分的预测应变量的结果。</p>
<p>其次一般特征选择的<strong>步骤</strong>为：</p>
<p>1：初步估计和排序</p>
<blockquote>
<p>a)对随机森林中的特征变量按照VI（Variable Importance）降序排序。</p>
<p>b)确定删除比例,从当前的特征变量中剔除相应比例不重要的指标，从而得到一个新的特征集。</p>
<p>c)用新的特征集建立新的随机森林,并计算特征集中每个特征的VI,并排序。</p>
<p>d)重复以上步骤,直到剩下m个特征。</p>
</blockquote>
<p>2：根据1中得到的每个特征集和它们建立起来的随机森林,计算对应的袋外误差率(OOB err),将袋外误差率最低的特征集作为最后选定的特征集。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/09/PCA的数学原理/" title="PCA的数学原理(转)" itemprop="url">PCA的数学原理(转)</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-09T12:22:31.986Z" itemprop="datePublished"> 发表于 2018-07-09</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>​       PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍PCA的基本数学原理，帮助读者了解PCA的工作机制是什么。</p>
<p>​       当然我并不打算把文章写成纯数学文章，而是希望用直观和易懂的方式叙述PCA的数学原理，所以整个文章不会引入严格的数学推导。希望读者在看完这篇文章后能更好的明白PCA的工作原理。</p>
<h3 id="数据的向量表示及降维问题"><a href="#数据的向量表示及降维问题" class="headerlink" title="数据的向量表示及降维问题"></a>数据的向量表示及降维问题</h3><p>​    一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下：</p>
<p>(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额)</p>
<p>​    其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子：</p>
<p><img src="http://120.25.221.136/images/pca/pca1.png" alt=""></p>
<p>​    注意这里我用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时我会省略转置符号，但我们说到向量默认都是指列向量。</p>
<p>​    我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。</p>
<p>​    降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。</p>
<p>​    <strong>举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。</strong></p>
<p>​    当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。</p>
<p>​    这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。</p>
<p>​    上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？</p>
<p>​    要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。</p>
<h3 id="向量的表示及基变换"><a href="#向量的表示及基变换" class="headerlink" title="向量的表示及基变换"></a>向量的表示及基变换</h3><p>既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。</p>
<h4 id="内积与投影"><a href="#内积与投影" class="headerlink" title="内积与投影"></a>内积与投影</h4><p>下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为：</p>
<p><img src="http://120.25.221.136/images/pca/pca2.png" alt=""></p>
<p>内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则<img src="http://120.25.221.136/images/pca/pcaa.png" alt="">则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：</p>
<p><img src="http://120.25.221.136/images/pca/pca3.png" alt=""></p>
<p>现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为<img src="http://120.25.221.136/images/pca/1.png" alt="">，其中<img src="http://120.25.221.136/images/pca/2.png" alt="">是向量A的模，也就是A线段的标量长度。</p>
<p>注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。</p>
<p>到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式：</p>
<p><img src="http://120.25.221.136/images/pca/pca5.png" alt=""></p>
<p>现在事情似乎是有点眉目了：A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让|B|=1|B|=1，那么就变成了：</p>
<p><img src="http://120.25.221.136/images/pca/pca6.png" alt=""></p>
<p>也就是说，<strong>设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度</strong>！这就是内积的一种几何解释，也是我们得到的第一个重要结论。在后面的推导中，将反复使用这个结论。</p>
<h3 id="基"><a href="#基" class="headerlink" title="基"></a>基</h3><p>下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量：</p>
<p><img src="http://120.25.221.136/images/pca/pca7.png" alt=""></p>
<p>在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。</p>
<p>不过我们常常忽略，<strong>只有一个(3,2)本身是不能够精确表示一个向量的</strong>。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。</p>
<p>更正式的说，向量(x,y)实际上表示线性组合：</p>
<p><img src="http://120.25.221.136/images/pca/pca8.png" alt=""></p>
<p>不难证明所有二维向量都可以表示为这样的线性组合。此处(1,0)和(0,1)叫做二维空间中的一组基。</p>
<p><img src="http://120.25.221.136/images/pca/pca9.png" alt=""></p>
<p>所以，<strong>要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了</strong>。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。</p>
<p>我们之所以默认选择(1,0)和(0,1)为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。</p>
<p>例如，(1,1)和(-1,1)也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为<img src="http://120.25.221.136/images/pca/pca10.png" alt=""></p>
<p>现在，我们想获得(3,2)在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为<img src="http://120.25.221.136/images/pca/pca11.png" alt="">。下图给出了新的基以及(3,2)在新基上坐标值的示意图：<img src="http://120.25.221.136/images/pca/pca12.png" alt=""></p>
<p>另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。</p>
<h4 id="基变换的矩阵表示"><a href="#基变换的矩阵表示" class="headerlink" title="基变换的矩阵表示"></a>基变换的矩阵表示</h4><p>下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将(3,2)变换为新基上的坐标，就是用(3,2)与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换：</p>
<p><img src="http://120.25.221.136/images/pca/pca13.png" alt=""></p>
<p>太漂亮了！其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示：</p>
<p><img src="http://120.25.221.136/images/pca/pca15.png" alt=""></p>
<p>于是一组向量的基变换被干净的表示为矩阵的相乘。</p>
<p><strong>一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果</strong>。</p>
<p>数学表示为：<img src="http://120.25.221.136/images/pca/pca16.png" alt=""></p>
<p>其中<img src="http://120.25.221.136/images/pca/pca17.png" alt="">是一个行向量，表示第i个基，<img src="http://120.25.221.136/images/pca/pca18.png" alt="">是一个列向量，表示第j个原始数据记录。</p>
<p>特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。</p>
<p>最后，上述分析同时给矩阵相乘找到了一种物理解释：<strong>两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去</strong>。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。</p>
<h3 id="协方差矩阵及优化目标"><a href="#协方差矩阵及优化目标" class="headerlink" title="协方差矩阵及优化目标"></a>协方差矩阵及优化目标</h3><p>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？</p>
<p>要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。</p>
<p>为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式：</p>
<p><img src="http://120.25.221.136/images/pca/pca19.png" alt=""></p>
<p>其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。</p>
<p>我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后：<img src="http://120.25.221.136/images/pca/pca20.png" alt=""></p>
<p>我们可以看下五条数据在平面直角坐标系内的样子：</p>
<p><img src="http://120.25.221.136/images/pca/pca21.png" alt=""></p>
<p>​    现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？</p>
<p>​    通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。</p>
<p>那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。</p>
<p>​    以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。</p>
<p>下面，我们用数学方法表述这个问题。</p>
<h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><p>上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即：<img src="http://120.25.221.136/images/pca/pca22.png" alt=""></p>
<p>由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：<img src="http://120.25.221.136/images/pca/pca88.png" alt=""></p>
<p>于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</p>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。</p>
<p>如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。</p>
<p>数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：</p>
<p><img src="http://120.25.221.136/images/pca/pca23.png" alt=""></p>
<p>可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。</p>
<p>当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。</p>
<p>至此，我们得到了降维问题的优化目标：<strong>将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）</strong>。</p>
<h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。</p>
<p>我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：</p>
<p>假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：<img src="http://120.25.221.136/images/pca/pca24.png" alt=""></p>
<p>然后我们用X乘以X的转置，并乘上系数1/m：<img src="http://120.25.221.136/images/pca/pca25.png" alt=""></p>
<p>奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。</p>
<p>根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：</p>
<p><strong>设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设</strong><img src="http://120.25.221.136/images/pca/pca89.png" alt=""><strong>，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差</strong>。</p>
<h4 id="协方差矩阵对角化"><a href="#协方差矩阵对角化" class="headerlink" title="协方差矩阵对角化"></a>协方差矩阵对角化</h4><p>根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p>
<p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：<img src="http://120.25.221.136/images/pca/pca26.png" alt=""></p>
<p>现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了<strong>寻找一个矩阵P，满足</strong><img src="http://120.25.221.136/images/pca/pca90.png" alt=""><strong>是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件</strong>。</p>
<p>至此，我们离“发明”PCA还有仅一步之遥！</p>
<p>现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。</p>
<p>由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：</p>
<p>1）实对称矩阵不同特征值对应的特征向量必然正交。</p>
<p>2）设特征向量λ重数为r，则必然存在r个线性无关的特征向量对应于λ，因此可以将这r个特征向量单位正交化。</p>
<p>由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为<img src="http://120.25.221.136/images/pca/pca27.png" alt="">我们将其按列组成矩阵：<img src="http://120.25.221.136/images/pca/pca27.png" alt=""></p>
<p>则对协方差矩阵C有如下结论：<img src="http://120.25.221.136/images/pca/pca29.png" alt="">其中Λ为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。</p>
<p>以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。</p>
<p>到这里，我们发现我们已经找到了需要的矩阵P：<img src="http://120.25.221.136/images/pca/pca30.png" alt="">P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照Λ中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p>
<p>至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。算法及实例</p>
<p>为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。</p>
<h4 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h4><p>总结一下PCA的算法步骤：</p>
<p>设有m条n维数据。</p>
<p>1）将原始数据按列组成n行m列矩阵X</p>
<p>2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</p>
<p>3）求出协方差矩阵<img src="http://120.25.221.136/images/pca/pca31.png" alt=""></p>
<p>4）求出协方差矩阵的特征值及对应的特征向量</p>
<p>5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</p>
<p>6）<img src="http://120.25.221.136/images/pca/pca32.png" alt="">即为降维到k维后的数据</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/08/深度学习权重初始化/" title="深度学习权重初始化(转)" itemprop="url">深度学习权重初始化(转)</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-08T05:32:19.166Z" itemprop="datePublished"> 发表于 2018-07-08</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>权值初始化的方法主要有：常量初始化（constant）、高斯分布初始化（gaussian）、positive_unitball初始化、均匀分布初始化（uniform）、xavier初始化、msra初始化、双线性初始化（bilinear）</p>
<p><strong>常量初始化(constant)</strong></p>
<p>​       把权值或者偏置初始化为一个常数，具体是什么常数，可以自己定义</p>
<p><strong>高斯分布初始化（gaussian）</strong></p>
<p>​        正态初始化   如：w = tf.Variable(tf.random_normal(shape, stddev=0.01))，缺点是：</p>
<p>标准差太大，容易梯度消失和梯度爆炸，合适的标准差在深层网络中，容易出现梯度弥散。</p>
<p>梯度弥散：</p>
<p>​        靠近输出层的hidden layer 梯度大，参数更新快，所以很快就会收敛；而靠近输入层的hidden layer 梯度小，参数更新慢，几乎就和初始状态一样，随机分布。这种现象就是梯度弥散（vanishing gradient problem）。而在另一种情况中，前面layer的梯度通过训练变大，而后面layer的梯度指数级增大，这种现象又叫做梯度爆炸(exploding gradient problem)。总的来说，就是在这个深度网络中，梯度相当不稳定(unstable)。可以通过relu等激活函数替代，可以加上maxout层。</p>
<p><strong>positive_unitball初始化</strong></p>
<p>​       让每一个神经元的输入的权值和为 1，例如：一个神经元有100个输入，让这100个输入的权值和为1.  首先给这100个权值赋值为在（0，1）之间的均匀分布，然后，每一个权值再除以它们的和就可以啦。这么做，可以有助于防止权值初始化过大，从而防止激活函数（sigmoid函数）进入饱和区。所以，它应该比较适合simgmoid形的激活函数</p>
<p><strong>均匀分布初始化（uniform）</strong></p>
<p>​       将权值与偏置进行均匀分布的初始化，用min 与 max 来控制它们的的上下限，默认为（0，1）</p>
<p><strong>xavier初始化</strong></p>
<p>​       对于权值的分布：均值为0，方差为（1 / 输入的个数） 的 均匀分布。如果我们更注重前向传播的话，我们可以选择 fan_in，即正向传播的输入个数；如果更注重后向传播的话，我们选择 fan_out, 因为在反向传播的时候，fan_out就是神经元的输入个数；如果两者都考虑的话，就选  average = (fan_in + fan_out) /2。对于ReLU激活函数来说，XavierFiller初始化也是很适合。关于该初始化方法，具体可以参考<a href="https://zhuanlan.zhihu.com/p/22028079" target="_blank" rel="noopener">文章1</a>、<a href="https://zhuanlan.zhihu.com/p/22044472" target="_blank" rel="noopener">文章2</a>，该方法假定激活函数是线性的。</p>
<p><strong>msra初始化</strong></p>
<p>​       对于权值的分布：基于均值为0，方差为( 2/输入的个数)的高斯分布；它特别适合 ReLU激活函数，该方法主要是基于Relu函数提出的，推导过程类似于xavier，可以参考<a href="http://blog.csdn.net/shuzfan/article/details/51347572" target="_blank" rel="noopener">博客</a>。</p>
<p><strong>双线性初始化（bilinear）</strong></p>
<p>​      常用在反卷积神经网络里的权值初始化</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/06/决策树算法/" title="决策树ID3/C4.5/CART算法比较" itemprop="url">决策树ID3/C4.5/CART算法比较</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-06T02:31:04.711Z" itemprop="datePublished"> 发表于 2018-07-06</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="决策树介绍"><a href="#决策树介绍" class="headerlink" title="决策树介绍"></a>决策树介绍</h3><p>​          决策树模型在监督学习中非常常见，可用于分类（二分类、多分类）和回归。虽然将多棵弱决策树的Bagging、Random Forest、Boosting等tree ensembel 模型更为常见，但是“完全生长”决策树因为其简单直观，具有很强的解释性，一般而言一棵“完全生长”的决策树包含，<strong>特征选择、决策树构建、剪枝</strong>三个过程。决策树是一种<strong>贪心算法</strong>，每次选取的分割数据的特征都是当前的最佳选择，并不关心是否达到最优。</p>
<h3 id="决策树的优缺点"><a href="#决策树的优缺点" class="headerlink" title="决策树的优缺点"></a>决策树的优缺点</h3><p>优点：</p>
<ol>
<li>决策树算法中学习简单的决策规则建立决策树模型的过程非常容易理解，</li>
<li>决策树模型可以可视化，非常直观</li>
<li>应用范围广，可用于分类和回归，而且非常容易做多类别的分类</li>
<li>能够处理数值型和连续的样本特征</li>
</ol>
<p>​    缺点：</p>
<ol>
<li>很容易在训练数据中生成复杂的树结构，造成过拟合（overfitting）。剪枝可以缓解过拟合的负作用，常用方法是限制树的高度、叶子节点中的最少样本数量。</li>
<li>学习一棵最优的决策树被认为是NP-Complete问题。实际中的决策树是基于启发式的贪心算法建立的，这种算法不能保证建立全局最优的决策树。Random Forest 引入随机能缓解这个问题</li>
</ol>
<h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>​        ID3决策树可以有多个分支，但是不能处理特征值为连续的情况。决策树是一种贪心算法，每次选取的分割数据的特征都是当前的最佳选择，并不关心是否达到最优。在ID3中，每次根据“最大信息熵增益”选取当前最佳的特征来分割数据，并按照该特征的所有取值来切分，也就是说如果一个特征有4种取值，数据将被切分4份，一旦按某特征切分后，该特征在之后的算法执行中，将不再起作用，所以有观点认为这种切分方式过于迅速。ID3算法十分简单，核心是根据“最大信息熵增益”原则选择划分当前数据集的最好特征，信息熵是信息论里面的概念，是信息的度量方式，不确定度越大或者说越混乱，熵就越大。在建立决策树的过程中，根据特征属性划分数据，使得原本“混乱”的数据的熵(混乱度)减少，按照不同特征划分数据熵减少的程度会不一样。在ID3中选择熵减少程度最大的特征来划分数据（贪心），也就是“最大信息熵增益”原则。</p>
<p>缺点：①切分过于迅速；</p>
<p>②不能直接处理特征值为连续的情况，只有事先将连续型特征转换成离散型，但这转换过程中会破坏连续型变量的内在性质;</p>
<p>③ </p>
<p><img src="http://120.25.221.136/images/dt/tree.png" alt=""></p>
<h4 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a><strong>C4.5算法</strong></h4><p>C4.5是Ross Quinlan在1993年在ID3的基础上改进而提出的。.ID3采用的信息增益度量存在一个缺点，它一般会优先选择有较多属性值的Feature,因为属性值多的Feature会有相对较大的信息增益?(信息增益反映的给定一个条件以后不确定性减少的程度,必然是分得越细的数据集确定性更高,也就是条件熵越小,信息增益越大).为了避免这个不足C4.5中是用信息增益比率(gain ratio)来作为选择分支的准则。信息增益比率通过引入一个被称作分裂信息(Split information)的项来惩罚取值较多的Feature。除此之外，C4.5还弥补了ID3中不能处理特征属性值连续的问题。<strong>信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。一般为原集合信息熵。</strong></p>
<p><img src="http://120.25.221.136/images/dt/tree2.png" alt=""></p>
<p><strong>C4.5算法能够处理不完整的数据，常用的处理方法有以下三种：</strong></p>
<ul>
<li>给缺失属性赋予最常见的值。</li>
<li>丢弃含有缺失值的样本。</li>
<li>根据节点的样例上该属性值出现的情况赋一个概率值。</li>
</ul>
<h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a><strong>CART算法</strong></h4><p>CART（Classification and Regression tree）分类回归树由L.Breiman,J.Friedman,R.Olshen和C.Stone于1984年提出。ID3中根据属性值分割数据，之后该特征不会再起作用，这种快速切割的方式会影响算法的准确率。CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。下图显示信息熵增益的一半，Gini指数，分类误差率三种评价指标非常接近。回归时使用均方差作为loss function。基尼系数的计算与信息熵增益的方式非常类似，公式如下：</p>
<p><img src="http://120.25.221.136/images/dt/tree3.png" alt=""></p>
<p><strong>三种方法对比：</strong></p>
<p><strong>ID3的缺点，倾向于选择水平数量较多的变量，可能导致训练得到一个庞大且深度浅的树；另外输入变量必须是分类变量（连续变量必须离散化）；最后无法处理空值。</strong></p>
<p><strong>C4.5选择了信息增益率替代信息增益。</strong></p>
<p><strong>CART以基尼系数替代熵；最小化不纯度而不是最大化信息增益。</strong></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/05/掌握Adaboost算法/" title="掌握Addaboost算法" itemprop="url">掌握Addaboost算法</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-05T09:13:00.522Z" itemprop="datePublished"> 发表于 2018-07-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="Adaboost介绍"><a href="#Adaboost介绍" class="headerlink" title="Adaboost介绍"></a>Adaboost介绍</h3><p>AdaBoost，是英文”Adaptive Boosting”（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。</p>
<h3 id="Adaboost算法步骤"><a href="#Adaboost算法步骤" class="headerlink" title="Adaboost算法步骤"></a>Adaboost算法步骤</h3><p>具体说来，整个Adaboost 迭代算法就3步：</p>
<ol>
<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li>
<li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li>
<li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。</li>
</ol>
<p>AdaBoost算法过程</p>
<p>​    给定训练数据集：<img src="http://120.25.221.136/images/adsboost/ada.jpg" alt="">，其中用于表示训练样本的类别标签<img src="http://120.25.221.136/images/adsboost/yi.jpg" alt="">，i=1,…,N。Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。</p>
<p>相关符号定义：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost2.jpg" alt=""></p>
<p>Adaboost的算法流程如下：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost4.jpg" alt=""></p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost5.jpg" alt=""></p>
<p>相关说明：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost7.jpg" alt=""></p>
<p>综合上面的推导，可得样本分错与分对时，其权值更新的公式为：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost8.jpg" alt=""></p>
<h3 id="AdaBoost实例讲解"><a href="#AdaBoost实例讲解" class="headerlink" title="AdaBoost实例讲解"></a>AdaBoost实例讲解</h3><p>例：给定如图所示的训练样本，弱分类器采用平行于坐标轴的直线，用Adaboost算法的实现强分类过程。</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost9.jpg" alt=""></p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost11.jpg" alt=""></p>
<p>数据分析：</p>
<p>   将这10个样本作为训练数据，根据X和Y的对应关系，可把这10个数据分为两类，图中用“+”表示类别1，用“O”表示类别-1。本例使用水平或者垂直的直线作为分类器，图中已经给出了三个弱分类器，即：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost12.jpg" alt=""></p>
<p>初始化：</p>
<p>   首先需要初始化训练样本数据的权值分布，每一个训练样本最开始时都被赋予相同的权值：wi=1/N，这样训练样本集的初始权值分布D1(i)：</p>
<p>   令每个权值w1i= 1/N = 0.1，其中，N = 10，i = 1,2, …, 10，然后分别对于t= 1,2,3, …等值进行迭代（t表示迭代次数，表示第t轮），下表已经给出训练样本的权值分布情况：</p>
<p><img src="http://120.25.221.136/images/adsboost/ada10.jpg" alt=""></p>
<p>第1次迭代t=1：</p>
<p>初试的权值分布D1为1/N（10个数据，每个数据的权值皆初始化为0.1），</p>
<p>D1=[0.1,  0.1, 0.1, 0.1, 0.1, 0.1,0.1, 0.1, 0.1, 0.1]</p>
<p>  在权值分布D1的情况下，取已知的三个弱分类器h1、h2和h3中误差率最小的分类器作为第1个基本分类器H1(x)（三个弱分类器的误差率都是0.3，那就取第1个吧）</p>
<p><img src="http://120.25.221.136/images/adsboost/ada11.jpg" alt=""></p>
<p>  在分类器H1(x)=h1情况下，样本点“5 7 8”被错分，因此基本分类器H1(x)的误差率为：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost13.jpg" alt=""></p>
<p>  可见，被误分类样本的权值之和影响误差率e，误差率e影响基本分类器在最终分类器中所占的权重α。</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost14.jpg" alt=""></p>
<p>  然后，更新训练样本数据的权值分布，用于下一轮迭代，对于正确分类的训练样本“1 2 3 4 6 9 10”（共7个）的权值更新为：<img src="http://120.25.221.136/images/adsboost/adaboost15.jpg" alt=""></p>
<p>  这样，第1轮迭代后，最后得到各个样本数据新的权值分布：</p>
<p>D2=[1/14,1/14,1/14,1/14,1/6,1/14,1/6,1/6,1/14,1/14]</p>
<p>由于样本数据“5 7 8”被H1(x)分错了，所以它们的权值由之前的0.1增大到1/6；反之，其它数据皆被分正确，所以它们的权值皆由之前的0.1减小到1/14，下表给出了权值分布的变换情况：</p>
<p>​    可得分类函数：f1(x)= α1H1(x) = 0.4236H1(x)。此时，组合一个基本分类器sign(f1(x))作为强分类器在训练数据集上有3个误分类点（即5 7 8），此时强分类器的训练错误为：0.3</p>
<p>第二次迭代t=2：</p>
<p>  在权值分布D2的情况下，再取三个弱分类器h1、h2和h3中误差率最小的分类器作为第2个基本分类器H2(x)：①　当取弱分类器h1=X1=2.5时，此时被错分的样本点为“5 7 8”：误差率e=1/6+1/6+1/6=3/6=1/2；</p>
<p>②　当取弱分类器h2=X1=8.5时，此时被错分的样本点为“3 4 6”：误差率e=1/14+1/14+1/14=3/14；③　当取弱分类器h3=X2=6.5时，此时被错分的样本点为“1 2 9”：误差率e=1/14+1/14+1/14=3/14；   因此，取当前最小的分类器h2作为第2个基本分类器H2(x)：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost17.jpg" alt=""></p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost18.jpg" alt=""></p>
<p>​    显然，H2(x)把样本“3 4 6”分错了，根据D2可知它们的权值为D2(3)=1/14，D2(4)=1/14， D2(6)=1/14，所以H2(x)在训练数据集上的误差率：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost19.jpg" alt=""></p>
<p>  这样，第2轮迭代后，最后得到各个样本数据新的权值分布：</p>
<p>D3=[1/22,1/22,1/6,1/6,7/66,1/6,7/66,7/66,1/22,1/22]</p>
<p>  下表给出了权值分布的变换情况：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost20.jpg" alt=""></p>
<p>   可得分类函数：f2(x)=0.4236H1(x) + 0.6496H2(x)。此时，组合两个基本分类器sign(f2(x))作为强分类器在训练数据集上有3个误分类点（即3 4 6），此时强分类器的训练错误为：0.3</p>
<p>第三次迭代t=3:</p>
<p>  在权值分布D3的情况下，再取三个弱分类器h1、h2和h3中误差率最小的分类器作为第3个基本分类器H3(x)：①　当取弱分类器h1=X1=2.5时，此时被错分的样本点为“5 7 8”：误差率e=7/66+7/66+7/66=7/22；</p>
<p>②　当取弱分类器h2=X1=8.5时，此时被错分的样本点为“3 4 6”：误差率e=1/6+1/6+1/6=1/2=0.5；</p>
<p>③　当取弱分类器h3=X2=6.5时，此时被错分的样本点为“1 2 9”：误差率e=1/22+1/22+1/22=3/22；</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost21.jpg" alt=""></p>
<p>   因此，取当前最小的分类器h3作为第3个基本分类器H3(x)：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost22.jpg" alt=""></p>
<p>  这样，第3轮迭代后，得到各个样本数据新的权值分布为：</p>
<p>D4=[1/6,1/6,11/114,11/114,7/114,11/114,7/114,7/114,1/6,1/38]</p>
<p>  下表给出了权值分布的变换情况：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost23.jpg" alt=""></p>
<p>​    可得分类函数：f3(x)=0.4236H1(x) + 0.6496H2(x)+0.9229H3(x)。此时，组合三个基本分类器sign(f3(x))作为强分类器，在训练数据集上有0个误分类点。至此，整个训练过程结束。</p>
<p>  整合所有分类器，可得最终的强分类器为：</p>
<p><img src="http://120.25.221.136/images/adsboost/adaboost24.jpg" alt=""></p>
<p><strong>这个强分类器Hfinal对训练样本的错误率为0！</strong></p>
<h3 id="AdaBoost的优点和缺点"><a href="#AdaBoost的优点和缺点" class="headerlink" title="AdaBoost的优点和缺点"></a>AdaBoost的优点和缺点</h3><p><strong>优点</strong></p>
<p>​     （1）Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也<strong>不存在过拟合的现象</strong>。<strong>(低偏差低方差)</strong></p>
<p>​     （2）Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。</p>
<p>​     （3）Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。</p>
<p>​     （4）Adaboost对同一个训练样本集训练不同的弱分类器，按照一定的方法把这些弱分类器集合起来，构造一个分类能力很强的强分类器，即“三个臭皮匠赛过一个诸葛亮”。</p>
<p><strong>缺点：</strong></p>
<p>​     在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。</p>
<p><strong>为什么AdaBoost不会过拟合？</strong></p>
<p>　　关于Adaboost，它是boosting算法，从bias-variance（偏差-方差）的角度来看，boosting算法主要关注的是降低偏差</p>
<p>　　而boosting算法每个分类器都是弱分类器，而弱分类器的特性就是high-bias &amp; low variance（高偏差-低方差），其与生俱来的优点就是泛化性能好</p>
<p>　　因此，将多个算法组合起来之后，可以达到降偏差的效果，进而得到一个偏差小、方差小的泛化能力好的模型。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/04/HIVE自定义函数之UDF，UDAF和UDTF区别/" title="HIVE自定义函数之UDF，UDAF和UDTF区别" itemprop="url">HIVE自定义函数之UDF，UDAF和UDTF区别</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-04T12:50:58.035Z" itemprop="datePublished"> 发表于 2018-07-04</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p> <strong>(1)udf(user defined function)：  自定义函数，特点是输入一行，输出一行</strong></p>
<p>  <strong>(2)udaf(user defined aggregation function)：自定义聚合函数，特点是输入多行，输出一行</strong></p>
<p>  <strong>(3)udtf(User-Defined Table-Generating Functions):自定义拆分函数，特点是输入一行，输出多行</strong></p>
<p>2、使用</p>
<p>​    (1)UDF开发和使用步骤</p>
<p>​       创建函数流程<br>a、自定义一个Java类<br>b、继承UDF类<br>c、重写evaluate方法<br>d、打成jar包<br>e、在hive执行add jar方法 (临时使用)<br>f、在hive执行创建模板函数<br>g、hql中使用</p>
<p>   (2)UDAF开发和使用步骤   </p>
<p>​           a、顶层UDAF类继承<br>            org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator里面编写嵌套类evaluator实现UDAF的逻辑。</p>
<p>​           b、实现resolver :resolver负责类型检查，操作符重载<br>                 resolver通常继承<br>                 org.apache.hadoop.hive.ql.udf.GenericUDAFResolver2，但是更建议继承AbstractGenericUDAFResolver，</p>
<p>​                  隔离将来hive接口的变化。<br>                 GenericUDAFResolver和GenericUDAFResolver2接口的区别是后面的允许evaluator实现可以访问更多的信息，</p>
<p>​                 例如DISTINCT限定符，通配符FUNCTION(*)。</p>
<p>​           c、实现evaluator :evaluator真正实现UDAF的逻辑<br>                 所有evaluators必须继承抽象类<br>                 org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator。予类必须实现它的一些抽象方法，</p>
<p>​                 实现UDAF的逻辑。</p>
<p>​           d、打成jar包<br>   e、在hive执行add jar方法 (临时使用)<br>   f、在hive执行创建模板函数<br>   g、hql中使用</p>
<p>​       (3)UDTF开发和使用</p>
<p>​         a、 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF,实现initialize, process, close三个方法。</p>
<p>  b、UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）。</p>
<p>  c、初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward()调用产生一行；</p>
<p>​       如果产生多列可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数。</p>
<p>​      最后close()方法调用，对需要清理的方法进行清理。</p>
<p>  d、打成jar包 </p>
<p>  e、在hive执行add jar方法 (临时使用)</p>
<p>  f、在hive执行创建模板函数 </p>
<p>  g、hql中使用</p>
<p>​     注意：UDTF有两种使用方法，一种直接放到select后面，一种和lateral view一起使用。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/04/hive 复合结构Map、Struct/" title="hive 复合结构Map、Struct(转)" itemprop="url">hive 复合结构Map、Struct(转)</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-04T07:37:08.038Z" itemprop="datePublished"> 发表于 2018-07-04</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Map详解</p>
<p>hive里支持map的结构如下：</p>
<p>(key1, value1, key2, value2, …) Creates a map with the given key/value pairs</p>
<p>建表语句：</p>
<p>create table test_map(name string, score map<string,int>)</string,int></p>
<p>ROW FORMAT DELIMITED</p>
<p>FIELDS TERMINATED BY ‘\t’</p>
<p>COLLECTION ITEMS TERMINATED BY ‘,’</p>
<p>MAP KEYS TERMINATED BY ‘:’</p>
<p>测试数据</p>
<p>cat test</p>
<p>leilei 数学:99,语文:90,英语:96</p>
<p>lucy 数学:100,语文:85,英语:91</p>
<p>将数据导入表中：</p>
<p>LOAD DATA LOCAL INPATH ‘/home/webopa/lei.wang/datas_test/test_map’ OVERWRITE INTO TABLE test_map;</p>
<p>在表中查询：</p>
<p>hive&gt; select * from test_map;</p>
<p>OK</p>
<p>leilei {“数学”:99,”语文”:90,”英语”:96}</p>
<p>lucy {“数学”:100,”语文”:85,”英语”:91}</p>
<p>Time taken: 0.052 seconds, Fetched: 2 row(s)</p>
<p>hive&gt; select ts.name,ts.score[‘数学’] from test_map ts;</p>
<p>Total jobs = 1</p>
<p>Launching Job 1 out of 1</p>
<p>…</p>
<p>Total MapReduce CPU Time Spent: 3 seconds 280 msec</p>
<p>OK</p>
<p>leilei 99</p>
<p>lucy 100</p>
<p>Time taken: 26.072 seconds, Fetched: 2 row(s)</p>
<p>Struct</p>
<p>hive里支持的Struct结构：</p>
<p>(val1, val2, val3, …) Creates a struct with the given field values. Struct field names will be col1, col2, …</p>
<p>建表语句</p>
<p>CREATE TABLE test_struct(name string,lable struct<price:string,pay:string,num:int>)</price:string,pay:string,num:int></p>
<p>ROW FORMAT DELIMITED</p>
<p>FIELDS TERMINATED BY ‘\t’</p>
<p>COLLECTION ITEMS TERMINATED BY ‘,’</p>
<p>测试数据</p>
<p>cat test_struct</p>
<p>aaa cheap,imm,1</p>
<p>bbb high,imm,2</p>
<p>将数据导入表中</p>
<p>LOAD DATA LOCAL INPATH ‘/home/webopa/lei.wang/datas_test/test_struct’ OVERWRITE INTO TABLE test_struct</p>
<p>在表中查询</p>
<p>hive&gt; select * from test_struct;</p>
<p>OK</p>
<p>aaa {“price”:”cheap”,”pay”:”imm”,”num”:1}</p>
<p>bbb {“price”:”high”,”pay”:”imm”,”num”:2}</p>
<p>Time taken: 0.046 seconds, Fetched: 2 row(s)</p>
<p>hive&gt; select name,lable.price from test_struct;</p>
<p>Total jobs = 1</p>
<p>Launching Job 1 out of 1</p>
<p>…</p>
<p>Total MapReduce CPU Time Spent: 1 seconds 270 msec</p>
<p>OK</p>
<p>aaa cheap</p>
<p>bbb high</p>
<p>Time taken: 20.054 seconds, Fetched: 2 row(s)</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/07/03/处理数据中的缺失值/" title="处理数据中的缺失值" itemprop="url">处理数据中的缺失值</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="PengYuan" target="_blank" itemprop="author">PengYuan</a>
		
  <p class="article-time">
    <time datetime="2018-07-03T02:12:53.412Z" itemprop="datePublished"> 发表于 2018-07-03</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="缺失值产生的原因"><a href="#缺失值产生的原因" class="headerlink" title="缺失值产生的原因"></a>缺失值产生的原因</h3><p>缺失值的产生的原因多种多样，主要分为机械原因和人为原因。</p>
<ul>
<li>机械原因是由于机械原因导致的数据收集或保存的失败造成的数据缺失，比如数据存储的失败，存储器损坏，机械故障导致某段时间数据未能收集（对于定时数据采集而言）。</li>
<li>人为原因是由于人的主观失误、历史局限或有意隐瞒造成的数据缺失，比如，在市场调查中被访人拒绝透露相关问题的答案，或者回答的问题是无效的，数据录入人员失误漏录了数据。 </li>
</ul>
<h3 id="处理缺失值的方法"><a href="#处理缺失值的方法" class="headerlink" title="处理缺失值的方法"></a>处理缺失值的方法</h3><p><strong>使用可用特征的均值来填补缺失值；</strong><br><strong> 使用特殊值来填补缺失值，如-1；</strong><br><strong> 忽略有缺失值的样本；</strong><br><strong> 使用相似样本的均值添补缺失值；</strong><br><strong> 使用另外的机器学习算法预测缺失值。</strong></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github 名片</p>
<div class="github-card" data-github="https://github.com/pengyuanqiuqiu" data-theme="medium"></div>
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>



  

  

  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="https://www.nowcoder.com/" target="_blank" title="牛客网">牛客网</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.acmcoder.com/index" target="_blank" title="赛码网">赛码网</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="http://www.163.com/rss/" target="_blank" title="rss">RSS 订阅</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="400" class="share_self"  frameborder="0" scrolling="no" src="https://widget.weibo.com/weiboshow/index.php?language=&width=0&height=400&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=1&isFans=0&uid=2871001534&verifier=2889e96f&colors=d6f3f7,ffffff,666666,0082cb,ecfbfd&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m jianyu. <br/>
			Let&#39;s go!</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/2871001534" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/pengyuanqiuqiu" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		<a href="http://www.zhihu.com/people/peng-yuan-19-57?utm_source=qq&amp;utm_medium=social" target="_blank" class="icon-zhihu" title="知乎"></a>
		
		
		
		<a href="mailto:987219258@qq.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by © 2018 
		
		<a href="https://pengyuanqiuqiu.github.io/" target="_blank" title="PengYuan">PengYuan</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>












<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
